# 大文件上传实践

> 大文件上传是前端项目中的常见需求，涉及文件切片、断点续传、进度监控等多项技术。本文详细介绍大文件上传的完整解决方案。

## 概述

大文件上传在现代Web应用中是一个重要的功能需求，特别是在文档管理、视频上传、数据导入等场景中。传统的文件上传方式在处理大文件时会遇到以下问题：

- **超时问题**：大文件传输时间长，容易超时
- **内存占用**：一次性上传大文件会占用大量内存
- **网络不稳定**：网络中断会导致重新上传
- **用户体验**：无法显示准确的上传进度

## 核心技术方案

### 1. 文件切片（File Chunking）

将大文件切分为多个小块，分别上传：

```javascript
/**
 * @description 文件切片工具类
 */
class FileChunker {
  constructor(file, chunkSize = 2 * 1024 * 1024) {
    this.file = file;
    this.chunkSize = chunkSize;
    this.totalChunks = Math.ceil(file.size / chunkSize);
  }

  /**
   * @description 创建文件切片，返回切片数组
   * 每个切片包含索引、数据、位置信息等详细信息
   */
  createChunks() {
    const chunks = [];                    // 存储所有切片的数组
    let start = 0;                       // 当前切片的起始位置

    for (let i = 0; i < this.totalChunks; i++) {
      const end = Math.min(start + this.chunkSize, this.file.size);  // 计算切片结束位置
      const chunk = this.file.slice(start, end);                     // 使用File.slice()创建切片

      chunks.push({
        index: i,          // 切片索引，用于标识切片顺序
        chunk: chunk,      // 切片数据对象
        start: start,      // 切片在原文件中的起始位置
        end: end,          // 切片在原文件中的结束位置
        size: chunk.size,  // 切片大小（字节）
        hash: null         // 切片hash值占位符，如需要可后续计算
      });

      start = end;  // 更新下一个切片的起始位置
    }

    return chunks;
  }

  /**
   * @description 计算整个文件的MD5 hash值
   * 注意：这里计算的是整个文件的hash，不是单个切片的hash
   * 用于文件唯一标识、去重检测、完整性验证、断点续传等功能
   * 使用Web Worker异步计算，避免阻塞主线程
   */
  async calculateFileHash() {
    return new Promise((resolve, reject) => {
      // 创建Web Worker来处理hash计算，避免阻塞UI
      const worker = new Worker('./js/hash-worker.js');

      // 向Worker发送文件和切片信息
      worker.postMessage({
        file: this.file,
        chunks: this.createChunks()
      });

      // 监听Worker返回的计算结果
      worker.onmessage = function(e) {
        const { hash } = e.data;
        resolve(hash);      // 返回计算得到的文件hash值
        worker.terminate(); // 计算完成后终止Worker
      };

      // 处理Worker计算过程中的错误
      worker.onerror = function(error) {
        reject(error);
        worker.terminate();
      };
    });
  }

  // ========== 分片Hash计算方法 ==========

  /**
   * @description 使用SparkMD5计算单个切片hash（推荐方法）
   * 优点：专为大文件设计，支持增量计算，性能更好，与整体文件hash算法一致
   *
   * 使用前需要引入SparkMD5库：
   * <script src="https://cdnjs.cloudflare.com/ajax/libs/spark-md5/3.0.2/spark-md5.min.js"></script>
   */
  async calculateChunkHash(chunk) {
    if (typeof SparkMD5 === 'undefined') {
      console.error('⚠️ 未检测到SparkMD5库，请先引入:');
      console.error('<script src="https://cdnjs.cloudflare.com/ajax/libs/spark-md5/3.0.2/spark-md5.min.js"></script>');
      throw new Error('SparkMD5库未加载');
    }

    return new Promise((resolve, reject) => {
      const reader = new FileReader();

      reader.onload = function(e) {
        try {
          const arrayBuffer = e.target.result;

          // 使用SparkMD5的ArrayBuffer方法，与文件hash计算保持一致
          const spark = new SparkMD5.ArrayBuffer();
          spark.append(arrayBuffer);           // 添加数据到hash计算器
          const hash = spark.end();            // 完成计算并获取结果

          console.log(`SparkMD5切片hash: ${hash}`);
          resolve(hash);
        } catch (error) {
          reject(error);
        }
      };

      reader.onerror = reject;
      // 关键：必须使用readAsArrayBuffer读取原始二进制数据
      reader.readAsArrayBuffer(chunk);
    });
  }

  /**
   * @description 创建包含hash的切片数组（可选功能）
   * 如果需要对每个切片进行完整性验证，可以使用此方法
   * 注意：计算所有切片的hash会增加处理时间
   */
  async createChunksWithHash(useHash = false) {
    const chunks = this.createChunks();

    if (!useHash) {
      console.log('跳过切片hash计算，提升性能');
      return chunks;
    }

    // 检查SparkMD5是否可用
    if (typeof SparkMD5 === 'undefined') {
      console.warn('⚠️ SparkMD5库未加载，跳过切片hash计算');
      console.warn('如需计算切片hash，请引入: <script src="https://cdnjs.cloudflare.com/ajax/libs/spark-md5/3.0.2/spark-md5.min.js"></script>');
      return chunks;
    }

    console.log('开始使用SparkMD5计算所有切片的hash值...');

    // 并行计算所有切片的hash（注意：可能占用大量CPU资源）
    const chunksWithHash = await Promise.all(
      chunks.map(async (chunkData, index) => {
        try {
          // 使用推荐的SparkMD5方法计算切片hash
          const hash = await this.calculateChunkHash(chunkData.chunk);
          console.log(`切片${index}hash计算完成: ${hash}`);

          return {
            ...chunkData,
            hash: hash  // 添加计算得到的hash值
          };
        } catch (error) {
          console.error(`切片${index}hash计算失败:`, error);
          return {
            ...chunkData,
            hash: null  // hash计算失败时设为null
          };
        }
      })
    );

    console.log('所有切片hash计算完成');
    return chunksWithHash;
  }

  /**
   * @description 验证前后端切片hash一致性的测试方法
   * 用于调试和验证前后端hash计算结果是否一致
   */
  async testChunkHashConsistency(chunkIndex = 0) {
    const chunks = this.createChunks();
    if (chunkIndex >= chunks.length) {
      throw new Error(`切片索引${chunkIndex}超出范围，总切片数：${chunks.length}`);
    }

    const chunk = chunks[chunkIndex];

    try {
      // 检查SparkMD5是否可用
      if (typeof SparkMD5 === 'undefined') {
        throw new Error('SparkMD5库未加载，无法进行hash一致性测试');
      }

      // 前端使用SparkMD5计算hash
      const frontendHash = await this.calculateChunkHash(chunk.chunk);
      console.log('前端SparkMD5计算的hash:', frontendHash);

      // 模拟上传到后端进行验证
      const formData = new FormData();
      formData.append('chunk', chunk.chunk);
      formData.append('chunkIndex', chunkIndex);
      formData.append('expectedHash', frontendHash);
      formData.append('hashAlgorithm', 'md5'); // 明确指定算法类型

      const response = await fetch('/api/test/chunk-hash', {
        method: 'POST',
        body: formData
      });

      const result = await response.json();

      console.log('后端计算的hash:', result.backendHash);
      console.log('hash是否一致:', result.isConsistent);

      if (!result.isConsistent) {
        console.error('⚠️ 前后端hash不一致，可能的原因:');
        console.error('1. 前后端使用了不同的hash算法');
        console.error('2. 数据在传输过程中被修改');
        console.error('3. 编码方式不同');
        console.error('4. 读取方式差异');
        console.error('5. 后端未使用MD5算法');
        console.error('前端算法: SparkMD5 (MD5)');
        console.error('后端算法:', result.algorithm || '未知');
      } else {
        console.log('✅ 前后端hash一致，切片完整性验证通过');
      }

      return result.isConsistent;
    } catch (error) {
      console.error('hash一致性测试失败:', error);
      return false;
    }
  }
}
```

### 2. Hash计算Worker

为了不阻塞主线程，使用Web Worker计算文件hash：

```javascript
// /public/workers/hash-worker.js
// 注意：这里计算的是整个文件的MD5哈希值，不是单个切片的hash
// 通过依次读取所有切片内容，增量计算整个文件的hash值
importScripts('/libs/spark-md5.min.js');

self.onmessage = function(e) {
  const { file, chunks } = e.data;

  // 创建MD5计算实例，用于增量计算整个文件的hash
  const spark = new SparkMD5.ArrayBuffer();
  const fileReader = new FileReader();
  let currentChunk = 0;

  /**
   * @description 依次加载每个切片进行hash计算
   * 这是一个增量计算过程：每个切片的内容都会append到同一个MD5计算实例中
   * 最终得到的是整个文件的hash值，而不是单个切片的hash
   */
  function loadNext() {
    if (currentChunk < chunks.length) {
      // 读取当前切片的内容
      fileReader.readAsArrayBuffer(chunks[currentChunk].chunk);
    } else {
      // 所有切片都已处理完毕，计算最终的文件hash
      const hash = spark.end();
      self.postMessage({ hash });
    }
  }

  fileReader.onload = function(e) {
    // 将当前切片的内容追加到MD5计算中
    // 这里是关键：spark.append()是增量计算，会将新数据添加到已有的计算中
    spark.append(e.target.result);
    currentChunk++;
    loadNext(); // 继续处理下一个切片
  };

  fileReader.onerror = function() {
    self.postMessage({ error: 'Hash calculation failed' });
  };

  loadNext(); // 开始处理第一个切片
};

/**
 * 工作原理说明：
 * 1. 接收文件的所有切片信息
 * 2. 依次读取每个切片的二进制内容
 * 3. 将每个切片的内容追加到SparkMD5实例中进行增量计算
 * 4. 处理完所有切片后，得到整个文件的MD5哈希值
 *
 * 优势：
 * - 避免一次性加载大文件到内存
 * - 在Worker线程中计算，不阻塞主线程
 * - 增量计算，内存使用稳定
 * - 计算结果与直接对整个文件计算hash完全相同
 */
```

### 3. 上传管理器

```javascript
/**
 * @description 大文件上传管理器
 * 负责管理文件切片、上传队列、并发控制、错误重试等功能
 */
class LargeFileUploader {
  /**
   * @param {Object} options 配置选项
   * @param {string} options.baseURL 上传接口基础URL
   * @param {number} options.chunkSize 切片大小（字节）
   * @param {number} options.concurrency 并发上传数量
   * @param {number} options.retryTimes 重试次数
   * @param {Function} options.onProgress 进度回调函数
   * @param {Function} options.onError 错误回调函数
   * @param {Function} options.onSuccess 成功回调函数
   */
  constructor(options = {}) {
    // 基础配置
    this.baseURL = options.baseURL || '/api/upload';              // 上传API的基础URL
    this.chunkSize = options.chunkSize || 2 * 1024 * 1024;       // 默认切片大小：2MB
    this.concurrency = options.concurrency || 3;                 // 默认并发数：3个切片同时上传
    this.retryTimes = options.retryTimes || 3;                   // 默认重试次数：3次

    // 事件回调函数
    this.onProgress = options.onProgress || (() => {});          // 上传进度回调
    this.onError = options.onError || (() => {});               // 错误处理回调
    this.onSuccess = options.onSuccess || (() => {});           // 成功完成回调

    // 内部状态管理
    this.uploadQueue = [];           // 待上传切片的队列
    this.uploadingCount = 0;         // 当前正在上传的切片数量
    this.uploadedChunks = new Set(); // 已成功上传的切片索引集合
    this.failedChunks = new Set();   // 上传失败的切片索引集合
  }

  /**
   * @description 上传文件的主流程
   * 包含文件切片、秒传检测、断点续传、并发上传、文件合并等完整流程
   * @param {File} file 要上传的文件对象
   * @param {Object} options 上传选项配置
   */
  async upload(file, options = {}) {
    try {
      // 步骤1: 创建文件切片
      // 将大文件分割成多个小的切片，便于分别上传和管理
      console.log('开始创建文件切片...');
      const chunker = new FileChunker(file, this.chunkSize);
      const chunks = chunker.createChunks();
      console.log(`文件切片完成，共生成 ${chunks.length} 个切片`);

      // 步骤2: 计算文件hash值
      // 通过计算整个文件的MD5值来唯一标识文件，用于去重和完整性校验
      console.log('开始计算文件hash...');
      const fileHash = await chunker.calculateFileHash();
      console.log(`文件hash计算完成: ${fileHash}`);

      // 步骤3: 检查文件是否已存在（实现秒传功能）
      // 如果服务器已有相同hash的文件，则直接返回成功，无需重新上传
      console.log('检查文件是否已存在...');
      const existsResult = await this.checkFileExists(fileHash, file.name);
      if (existsResult.exists) {
        console.log('文件已存在，执行秒传');
        this.onSuccess({
          fileHash,
          fileName: file.name,
          fileSize: file.size,
          message: '文件秒传成功'
        });
        return;
      }

      // 步骤4: 获取已上传的切片信息（实现断点续传功能）
      // 查询服务器上已经上传成功的切片，避免重复上传
      console.log('获取已上传的切片信息...');
      const uploadedChunks = await this.getUploadedChunks(fileHash);
      this.uploadedChunks = new Set(uploadedChunks);
      console.log(`发现 ${uploadedChunks.length} 个已上传的切片`);

      // 步骤5: 准备上传队列
      // 过滤掉已上传的切片，只上传缺失的部分
      this.uploadQueue = chunks
        .filter(chunk => !this.uploadedChunks.has(chunk.index))  // 过滤已上传的切片
        .map(chunk => ({
          ...chunk,
          fileHash,                    // 添加文件hash标识
          fileName: file.name,         // 添加文件名
          totalChunks: chunks.length,  // 添加总切片数
          retryCount: 0               // 初始化重试计数
        }));
      console.log(`准备上传 ${this.uploadQueue.length} 个切片`);

      // 步骤6: 开始并发上传切片
      // 使用并发控制机制上传所有待上传的切片
      console.log('开始上传切片...');
      await this.startUpload();
      console.log('所有切片上传完成');

      // 步骤7: 合并文件切片
      // 在服务器端将所有切片合并成完整的文件
      console.log('开始合并文件切片...');
      await this.mergeChunks(fileHash, file.name, chunks.length);
      console.log('文件合并完成');

      // 上传成功，触发成功回调
      this.onSuccess({
        fileHash,
        fileName: file.name,
        fileSize: file.size,
        message: '文件上传成功'
      });

    } catch (error) {
      // 捕获并处理上传过程中的任何错误
      console.error('文件上传失败:', error);
      this.onError(error);
    }
  }

  /**
   * @description 检查文件是否已存在
   */
  async checkFileExists(fileHash, fileName) {
    const response = await fetch(`${this.baseURL}/check`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ fileHash, fileName })
    });
    return response.json();
  }

  /**
   * @description 获取已上传的切片
   */
  async getUploadedChunks(fileHash) {
    const response = await fetch(`${this.baseURL}/chunks/${fileHash}`);
    const result = await response.json();
    return result.uploadedChunks || [];
  }

  /**
   * @description 开始上传切片
   */
  async startUpload() {
    return new Promise((resolve, reject) => {
      this.resolveUpload = resolve;
      this.rejectUpload = reject;

      // 启动并发上传
      for (let i = 0; i < this.concurrency; i++) {
        this.uploadNext();
      }
    });
  }

  /**
   * @description 上传下一个切片
   */
  async uploadNext() {
    if (this.uploadQueue.length === 0) {
      this.uploadingCount--;
      if (this.uploadingCount === 0) {
        this.resolveUpload();
      }
      return;
    }

    const chunkData = this.uploadQueue.shift();
    this.uploadingCount++;

    try {
      await this.uploadChunk(chunkData);
      this.uploadedChunks.add(chunkData.index);
      this.updateProgress();
      this.uploadNext();
    } catch (error) {
      await this.handleChunkError(chunkData, error);
    }
  }

  /**
   * @description 上传单个切片
   */
  async uploadChunk(chunkData) {
    const formData = new FormData();
    formData.append('chunk', chunkData.chunk);
    formData.append('fileHash', chunkData.fileHash);
    formData.append('fileName', chunkData.fileName);
    formData.append('chunkIndex', chunkData.index);
    formData.append('totalChunks', chunkData.totalChunks);

    const response = await fetch(`${this.baseURL}/chunk`, {
      method: 'POST',
      body: formData
    });

    if (!response.ok) {
      throw new Error(`Upload failed: ${response.statusText}`);
    }

    return response.json();
  }

  /**
   * @description 处理切片上传错误
   */
  async handleChunkError(chunkData, error) {
    chunkData.retryCount++;

    if (chunkData.retryCount <= this.retryTimes) {
      // 重试
      await new Promise(resolve => setTimeout(resolve, 1000 * chunkData.retryCount));
      this.uploadQueue.unshift(chunkData);
    } else {
      // 超过重试次数
      this.failedChunks.add(chunkData.index);
      this.rejectUpload(new Error(`Chunk ${chunkData.index} upload failed after ${this.retryTimes} retries`));
    }

    this.uploadNext();
  }

  /**
   * @description 更新上传进度
   */
  updateProgress() {
    const totalChunks = this.uploadedChunks.size + this.uploadQueue.length + this.failedChunks.size;
    const uploadedChunks = this.uploadedChunks.size;
    const progress = (uploadedChunks / totalChunks) * 100;

    this.onProgress({
      progress,
      uploadedChunks,
      totalChunks,
      failedChunks: this.failedChunks.size
    });
  }

  /**
   * @description 合并文件切片
   */
  async mergeChunks(fileHash, fileName, totalChunks) {
    const response = await fetch(`${this.baseURL}/merge`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        fileHash,
        fileName,
        totalChunks
      })
    });

    if (!response.ok) {
      throw new Error('File merge failed');
    }

    return response.json();
  }

  /**
   * @description 暂停上传
   */
  pause() {
    this.isPaused = true;
    this.uploadQueue = [];
  }

  /**
   * @description 恢复上传
   */
  resume() {
    this.isPaused = false;
    this.startUpload();
  }

  /**
   * @description 取消上传
   */
  cancel() {
    this.isPaused = true;
    this.uploadQueue = [];
    this.uploadedChunks.clear();
    this.failedChunks.clear();
  }
}
```

## 使用示例

### 1. 基础使用

```javascript
// 创建上传器实例
const uploader = new LargeFileUploader({
  baseURL: '/api/upload',
  chunkSize: 2 * 1024 * 1024, // 2MB
  concurrency: 3,
  retryTimes: 3,
  onProgress: (progressInfo) => {
    console.log(`上传进度: ${progressInfo.progress.toFixed(2)}%`);
    updateProgressBar(progressInfo.progress);
  },
  onError: (error) => {
    console.error('上传失败:', error);
    showErrorMessage(error.message);
  },
  onSuccess: (result) => {
    console.log('上传成功:', result);
    showSuccessMessage('文件上传成功！');
  }
});

// 文件选择处理
document.getElementById('fileInput').addEventListener('change', async (event) => {
  const file = event.target.files[0];
  if (!file) return;

  // 文件大小检查
  const maxSize = 500 * 1024 * 1024; // 500MB
  if (file.size > maxSize) {
    alert('文件大小不能超过500MB');
    return;
  }

  // 开始上传
  try {
    await uploader.upload(file);
  } catch (error) {
    console.error('上传过程中出错:', error);
  }
});
```

### 2. Vue组件实现

```vue
<template>
  <div class="large-file-uploader">
    <div class="upload-area" @click="selectFile" @drop="handleDrop" @dragover.prevent>
      <input ref="fileInput" type="file" @change="handleFileSelect" style="display: none">
      <div v-if="!uploading" class="upload-placeholder">
        <i class="upload-icon">📁</i>
        <p>点击选择文件或拖拽文件到此处</p>
        <p class="upload-tip">支持最大500MB的文件上传</p>
      </div>
      <div v-else class="upload-progress">
        <div class="progress-info">
          <h3>{{ currentFile.name }}</h3>
          <p>{{ formatFileSize(currentFile.size) }}</p>
        </div>
        <div class="progress-bar">
          <div class="progress-fill" :style="{ width: progress + '%' }"></div>
        </div>
        <div class="progress-text">
          {{ progress.toFixed(1) }}% ({{ uploadedChunks }}/{{ totalChunks }})
        </div>
        <div class="upload-controls">
          <button @click="pauseUpload" v-if="!isPaused">暂停</button>
          <button @click="resumeUpload" v-if="isPaused">继续</button>
          <button @click="cancelUpload">取消</button>
        </div>
      </div>
    </div>
  </div>
</template>

<script>
import { LargeFileUploader } from '@/utils/large-file-uploader';

export default {
  name: 'LargeFileUploader',
  data() {
    return {
      uploader: null,
      uploading: false,
      isPaused: false,
      progress: 0,
      uploadedChunks: 0,
      totalChunks: 0,
      currentFile: null
    };
  },
  mounted() {
    this.initUploader();
  },
  methods: {
    initUploader() {
      this.uploader = new LargeFileUploader({
        baseURL: '/api/upload',
        chunkSize: 2 * 1024 * 1024,
        concurrency: 3,
        retryTimes: 3,
        onProgress: this.handleProgress,
        onError: this.handleError,
        onSuccess: this.handleSuccess
      });
    },

    selectFile() {
      this.$refs.fileInput.click();
    },

    handleFileSelect(event) {
      const file = event.target.files[0];
      if (file) {
        this.startUpload(file);
      }
    },

    handleDrop(event) {
      event.preventDefault();
      const files = event.dataTransfer.files;
      if (files.length > 0) {
        this.startUpload(files[0]);
      }
    },

    async startUpload(file) {
      // 文件验证
      if (!this.validateFile(file)) {
        return;
      }

      this.currentFile = file;
      this.uploading = true;
      this.progress = 0;
      this.uploadedChunks = 0;
      this.totalChunks = 0;

      try {
        await this.uploader.upload(file);
      } catch (error) {
        console.error('上传失败:', error);
      }
    },

    validateFile(file) {
      const maxSize = 500 * 1024 * 1024; // 500MB
      const allowedTypes = [
        'video/mp4', 'video/avi', 'video/mov',
        'application/pdf', 'application/zip',
        'image/jpeg', 'image/png'
      ];

      if (file.size > maxSize) {
        this.$message.error('文件大小不能超过500MB');
        return false;
      }

      if (!allowedTypes.includes(file.type)) {
        this.$message.error('不支持的文件类型');
        return false;
      }

      return true;
    },

    handleProgress(progressInfo) {
      this.progress = progressInfo.progress;
      this.uploadedChunks = progressInfo.uploadedChunks;
      this.totalChunks = progressInfo.totalChunks;
    },

    handleError(error) {
      this.uploading = false;
      this.$message.error(`上传失败: ${error.message}`);
    },

    handleSuccess(result) {
      this.uploading = false;
      this.$message.success('文件上传成功！');
      this.$emit('upload-success', result);
    },

    pauseUpload() {
      this.uploader.pause();
      this.isPaused = true;
    },

    resumeUpload() {
      this.uploader.resume();
      this.isPaused = false;
    },

    cancelUpload() {
      this.uploader.cancel();
      this.uploading = false;
      this.isPaused = false;
      this.currentFile = null;
    },

    formatFileSize(bytes) {
      if (bytes === 0) return '0 Bytes';
      const k = 1024;
      const sizes = ['Bytes', 'KB', 'MB', 'GB'];
      const i = Math.floor(Math.log(bytes) / Math.log(k));
      return parseFloat((bytes / Math.pow(k, i)).toFixed(2)) + ' ' + sizes[i];
    }
  }
};
</script>

<style scoped>
.large-file-uploader {
  width: 100%;
  max-width: 600px;
  margin: 0 auto;
}

.upload-area {
  border: 2px dashed #d9d9d9;
  border-radius: 8px;
  padding: 40px;
  text-align: center;
  cursor: pointer;
  transition: border-color 0.3s;
}

.upload-area:hover {
  border-color: #1890ff;
}

.upload-placeholder .upload-icon {
  font-size: 48px;
  margin-bottom: 16px;
  display: block;
}

.upload-tip {
  color: #999;
  font-size: 14px;
  margin-top: 8px;
}

.upload-progress {
  text-align: left;
}

.progress-info h3 {
  margin: 0 0 8px 0;
  font-size: 16px;
}

.progress-info p {
  margin: 0;
  color: #666;
  font-size: 14px;
}

.progress-bar {
  width: 100%;
  height: 8px;
  background-color: #f0f0f0;
  border-radius: 4px;
  margin: 16px 0;
  overflow: hidden;
}

.progress-fill {
  height: 100%;
  background-color: #1890ff;
  transition: width 0.3s;
}

.progress-text {
  font-size: 14px;
  color: #666;
  margin-bottom: 16px;
}

.upload-controls button {
  margin-right: 8px;
  padding: 6px 16px;
  border: 1px solid #d9d9d9;
  border-radius: 4px;
  background: white;
  cursor: pointer;
}

.upload-controls button:hover {
  border-color: #1890ff;
  color: #1890ff;
}
</style>

## 后端实现

### 1. Node.js + Express 实现

```javascript
/**
 * @description 大文件上传后端实现
 */
const express = require('express');
const multer = require('multer');
const fs = require('fs-extra');
const path = require('path');
const crypto = require('crypto');
const { promisify } = require('util');

const app = express();
const upload = multer({ dest: 'temp/' });

// 配置
const config = {
  uploadDir: './uploads',
  tempDir: './temp',
  chunkDir: './chunks'
};

// 确保目录存在
fs.ensureDirSync(config.uploadDir);
fs.ensureDirSync(config.tempDir);
fs.ensureDirSync(config.chunkDir);

/**
 * @description 检查文件是否已存在（秒传）
 */
app.post('/api/upload/check', async (req, res) => {
  try {
    const { fileHash, fileName } = req.body;
    const filePath = path.join(config.uploadDir, fileName);

    if (await fs.pathExists(filePath)) {
      // 验证文件hash
      const existingHash = await calculateFileHash(filePath);
      if (existingHash === fileHash) {
        return res.json({
          exists: true,
          message: '文件已存在，支持秒传'
        });
      }
    }

    res.json({ exists: false });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

/**
 * @description 获取已上传的切片列表
 */
app.get('/api/upload/chunks/:fileHash', async (req, res) => {
  try {
    const { fileHash } = req.params;
    const chunkDir = path.join(config.chunkDir, fileHash);

    if (!(await fs.pathExists(chunkDir))) {
      return res.json({ uploadedChunks: [] });
    }

    const files = await fs.readdir(chunkDir);
    const uploadedChunks = files
      .filter(file => file.endsWith('.chunk'))
      .map(file => parseInt(file.split('.')[0]))
      .sort((a, b) => a - b);

    res.json({ uploadedChunks });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

/**
 * @description 上传单个切片
 */
app.post('/api/upload/chunk', upload.single('chunk'), async (req, res) => {
  try {
    const { fileHash, fileName, chunkIndex, totalChunks, chunkHash } = req.body;
    const chunkFile = req.file;

    if (!chunkFile) {
      return res.status(400).json({ error: '切片文件缺失' });
    }

    // 如果前端提供了切片hash，进行验证
    if (chunkHash) {
      const calculatedHash = await calculateFileHash(chunkFile.path);
      if (calculatedHash !== chunkHash) {
        // 清理临时文件
        await fs.remove(chunkFile.path);
        return res.status(400).json({
          error: '切片hash验证失败',
          expected: chunkHash,
          actual: calculatedHash
        });
      }
      console.log(`切片${chunkIndex}hash验证通过: ${calculatedHash}`);
    }

    // 创建切片存储目录
    const chunkDir = path.join(config.chunkDir, fileHash);
    await fs.ensureDir(chunkDir);

    // 保存切片
    const chunkPath = path.join(chunkDir, `${chunkIndex}.chunk`);
    await fs.move(chunkFile.path, chunkPath);

    // 记录切片信息
    const chunkInfo = {
      index: parseInt(chunkIndex),
      size: chunkFile.size,
      hash: chunkHash || null,          // 保存切片hash（如果有）
      uploadTime: new Date().toISOString()
    };

    const infoPath = path.join(chunkDir, `${chunkIndex}.info`);
    await fs.writeJson(infoPath, chunkInfo);

    res.json({
      success: true,
      chunkIndex: parseInt(chunkIndex),
      hash: chunkHash || null,
      message: '切片上传成功'
    });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

/**
 * @description 合并文件切片
 */
app.post('/api/upload/merge', async (req, res) => {
  try {
    const { fileHash, fileName, totalChunks } = req.body;
    const chunkDir = path.join(config.chunkDir, fileHash);
    const outputPath = path.join(config.uploadDir, fileName);

    // 检查所有切片是否存在
    const missingChunks = [];
    for (let i = 0; i < totalChunks; i++) {
      const chunkPath = path.join(chunkDir, `${i}.chunk`);
      if (!(await fs.pathExists(chunkPath))) {
        missingChunks.push(i);
      }
    }

    if (missingChunks.length > 0) {
      return res.status(400).json({
        error: '切片不完整',
        missingChunks
      });
    }

    // 合并文件
    const writeStream = fs.createWriteStream(outputPath);

    for (let i = 0; i < totalChunks; i++) {
      const chunkPath = path.join(chunkDir, `${i}.chunk`);
      const chunkBuffer = await fs.readFile(chunkPath);
      writeStream.write(chunkBuffer);
    }

    writeStream.end();

    // 等待写入完成
    await new Promise((resolve, reject) => {
      writeStream.on('finish', resolve);
      writeStream.on('error', reject);
    });

    // 验证合并后的文件
    const mergedHash = await calculateFileHash(outputPath);
    if (mergedHash !== fileHash) {
      await fs.remove(outputPath);
      throw new Error('文件合并后hash不匹配');
    }

    // 清理切片文件
    await fs.remove(chunkDir);

    res.json({
      success: true,
      fileName,
      fileSize: (await fs.stat(outputPath)).size,
      fileHash: mergedHash,
      message: '文件合并成功'
    });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

/**
 * @description 计算文件hash（与SparkMD5保持一致）
 * 确保使用MD5算法，与前端SparkMD5计算结果一致
 */
async function calculateFileHash(filePath) {
  // 使用MD5算法，确保与前端SparkMD5一致
  const hash = crypto.createHash('md5');
  const stream = fs.createReadStream(filePath);

  return new Promise((resolve, reject) => {
    stream.on('data', data => hash.update(data));
    stream.on('end', () => {
      const result = hash.digest('hex');
      console.log(`文件hash计算完成 (MD5): ${result}`);
      resolve(result);
    });
    stream.on('error', reject);
  });
}

/**
 * @description 清理过期的临时文件
 */
async function cleanupExpiredFiles() {
  const expireTime = 24 * 60 * 60 * 1000; // 24小时
  const now = Date.now();

  try {
    const chunkDirs = await fs.readdir(config.chunkDir);

    for (const dir of chunkDirs) {
      const dirPath = path.join(config.chunkDir, dir);
      const stat = await fs.stat(dirPath);

      if (now - stat.mtime.getTime() > expireTime) {
        await fs.remove(dirPath);
        console.log(`清理过期目录: ${dirPath}`);
      }
    }
  } catch (error) {
    console.error('清理过期文件失败:', error);
  }
}

// 定时清理过期文件
setInterval(cleanupExpiredFiles, 60 * 60 * 1000); // 每小时执行一次

app.listen(3000, () => {
  console.log('大文件上传服务启动在端口 3000');
});

/**
 * @description 测试切片hash一致性的API
 * 用于验证前后端hash计算结果是否一致
 */
app.post('/api/test/chunk-hash', upload.single('chunk'), async (req, res) => {
  try {
    const { expectedHash, chunkIndex, hashAlgorithm = 'md5' } = req.body;
    const chunkFile = req.file;

    if (!chunkFile) {
      return res.status(400).json({ error: '切片文件缺失' });
    }

    // 后端计算切片hash（确保使用相同的MD5算法）
    const backendHash = await calculateFileHash(chunkFile.path);

    // 比较hash值
    const isConsistent = backendHash === expectedHash;

    // 清理临时文件
    await fs.remove(chunkFile.path);

    res.json({
      isConsistent: isConsistent,
      frontendHash: expectedHash,
      backendHash: backendHash,
      algorithm: 'md5',
      chunkIndex: parseInt(chunkIndex),
      message: isConsistent ? 'Hash一致' : 'Hash不一致'
    });

  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});
```

### 2. Spring Boot 实现

```java
/**
 * @description 大文件上传控制器
 */
@RestController
@RequestMapping("/api/upload")
public class LargeFileUploadController {

    private final LargeFileUploadService uploadService;

    @Value("${upload.chunk.path}")
    private String chunkPath;

    @Value("${upload.file.path}")
    private String filePath;

    /**
     * 检查文件是否已存在
     */
    @PostMapping("/check")
    public ResponseEntity<?> checkFile(@RequestBody CheckFileRequest request) {
        try {
            boolean exists = uploadService.checkFileExists(request.getFileHash(), request.getFileName());
            return ResponseEntity.ok(Map.of(
                "exists", exists,
                "message", exists ? "文件已存在，支持秒传" : "文件不存在"
            ));
        } catch (Exception e) {
            return ResponseEntity.status(500).body(Map.of("error", e.getMessage()));
        }
    }

    /**
     * 获取已上传的切片
     */
    @GetMapping("/chunks/{fileHash}")
    public ResponseEntity<?> getUploadedChunks(@PathVariable String fileHash) {
        try {
            List<Integer> uploadedChunks = uploadService.getUploadedChunks(fileHash);
            return ResponseEntity.ok(Map.of("uploadedChunks", uploadedChunks));
        } catch (Exception e) {
            return ResponseEntity.status(500).body(Map.of("error", e.getMessage()));
        }
    }

    /**
     * 上传切片
     */
    @PostMapping("/chunk")
    public ResponseEntity<?> uploadChunk(
            @RequestParam("chunk") MultipartFile chunk,
            @RequestParam("fileHash") String fileHash,
            @RequestParam("fileName") String fileName,
            @RequestParam("chunkIndex") Integer chunkIndex,
            @RequestParam("totalChunks") Integer totalChunks) {
        try {
            uploadService.saveChunk(chunk, fileHash, chunkIndex);
            return ResponseEntity.ok(Map.of(
                "success", true,
                "chunkIndex", chunkIndex,
                "message", "切片上传成功"
            ));
        } catch (Exception e) {
            return ResponseEntity.status(500).body(Map.of("error", e.getMessage()));
        }
    }

    /**
     * 合并文件
     */
    @PostMapping("/merge")
    public ResponseEntity<?> mergeFile(@RequestBody MergeFileRequest request) {
        try {
            String filePath = uploadService.mergeChunks(
                request.getFileHash(),
                request.getFileName(),
                request.getTotalChunks()
            );

            return ResponseEntity.ok(Map.of(
                "success", true,
                "fileName", request.getFileName(),
                "filePath", filePath,
                "message", "文件合并成功"
            ));
        } catch (Exception e) {
            return ResponseEntity.status(500).body(Map.of("error", e.getMessage()));
        }
    }
}

/**
 * @description 大文件上传服务实现
 */
@Service
public class LargeFileUploadService {

    @Value("${upload.chunk.path}")
    private String chunkBasePath;

    @Value("${upload.file.path}")
    private String fileBasePath;

    /**
     * 检查文件是否已存在
     */
    public boolean checkFileExists(String fileHash, String fileName) throws IOException {
        Path filePath = Paths.get(fileBasePath, fileName);
        if (!Files.exists(filePath)) {
            return false;
        }

        // 验证文件hash
        String existingHash = calculateFileHash(filePath);
        return fileHash.equals(existingHash);
    }

    /**
     * 获取已上传的切片列表
     */
    public List<Integer> getUploadedChunks(String fileHash) throws IOException {
        Path chunkDir = Paths.get(chunkBasePath, fileHash);
        if (!Files.exists(chunkDir)) {
            return Collections.emptyList();
        }

        return Files.list(chunkDir)
            .filter(path -> path.toString().endsWith(".chunk"))
            .map(path -> {
                String fileName = path.getFileName().toString();
                return Integer.parseInt(fileName.split("\\.")[0]);
            })
            .sorted()
            .collect(Collectors.toList());
    }

    /**
     * 保存切片
     */
    public void saveChunk(MultipartFile chunk, String fileHash, Integer chunkIndex) throws IOException {
        Path chunkDir = Paths.get(chunkBasePath, fileHash);
        Files.createDirectories(chunkDir);

        Path chunkFile = chunkDir.resolve(chunkIndex + ".chunk");
        Files.copy(chunk.getInputStream(), chunkFile, StandardCopyOption.REPLACE_EXISTING);

        // 保存切片信息
        ChunkInfo chunkInfo = new ChunkInfo();
        chunkInfo.setIndex(chunkIndex);
        chunkInfo.setSize(chunk.getSize());
        chunkInfo.setUploadTime(LocalDateTime.now());

        Path infoFile = chunkDir.resolve(chunkIndex + ".info");
        ObjectMapper mapper = new ObjectMapper();
        mapper.writeValue(infoFile.toFile(), chunkInfo);
    }

    /**
     * 合并文件切片
     */
    public String mergeChunks(String fileHash, String fileName, Integer totalChunks) throws IOException {
        Path chunkDir = Paths.get(chunkBasePath, fileHash);
        Path outputFile = Paths.get(fileBasePath, fileName);

        // 检查切片完整性
        List<Integer> missingChunks = new ArrayList<>();
        for (int i = 0; i < totalChunks; i++) {
            Path chunkFile = chunkDir.resolve(i + ".chunk");
            if (!Files.exists(chunkFile)) {
                missingChunks.add(i);
            }
        }

        if (!missingChunks.isEmpty()) {
            throw new RuntimeException("切片不完整，缺失: " + missingChunks);
        }

        // 合并文件
        try (FileChannel outputChannel = FileChannel.open(outputFile,
                StandardOpenOption.CREATE, StandardOpenOption.WRITE)) {

            for (int i = 0; i < totalChunks; i++) {
                Path chunkFile = chunkDir.resolve(i + ".chunk");
                try (FileChannel chunkChannel = FileChannel.open(chunkFile, StandardOpenOption.READ)) {
                    chunkChannel.transferTo(0, chunkChannel.size(), outputChannel);
                }
            }
        }

        // 验证合并后的文件
        String mergedHash = calculateFileHash(outputFile);
        if (!fileHash.equals(mergedHash)) {
            Files.deleteIfExists(outputFile);
            throw new RuntimeException("文件合并后hash不匹配");
        }

        // 清理切片文件
        deleteDirectory(chunkDir);

        return outputFile.toString();
    }

    /**
     * 计算文件hash
     */
    private String calculateFileHash(Path filePath) throws IOException {
        MessageDigest md5 = MessageDigest.getInstance("MD5");
        try (InputStream is = Files.newInputStream(filePath)) {
            byte[] buffer = new byte[8192];
            int bytesRead;
            while ((bytesRead = is.read(buffer)) != -1) {
                md5.update(buffer, 0, bytesRead);
            }
        }

        byte[] hashBytes = md5.digest();
        return DatatypeConverter.printHexBinary(hashBytes).toLowerCase();
    }
}
```

## 性能优化策略

### 1. 切片大小优化

```javascript
/**
 * @description 动态计算最优切片大小
 */
class OptimalChunkCalculator {
  constructor() {
    this.networkSpeed = 0; // KB/s
    this.avgLatency = 0; // ms
    this.concurrency = 3;
  }

  /**
   * @description 测试网络速度
   */
  async measureNetworkSpeed() {
    const testSize = 100 * 1024; // 100KB测试文件
    const iterations = 3;
    let totalSpeed = 0;
    let totalLatency = 0;

    for (let i = 0; i < iterations; i++) {
      const start = performance.now();

      try {
        const response = await fetch('/api/speed-test', {
          method: 'POST',
          body: new ArrayBuffer(testSize)
        });

        const end = performance.now();
        const latency = end - start;
        const speed = testSize / (latency / 1000) / 1024; // KB/s

        totalSpeed += speed;
        totalLatency += latency;
      } catch (error) {
        console.warn('网络速度测试失败:', error);
      }
    }

    this.networkSpeed = totalSpeed / iterations;
    this.avgLatency = totalLatency / iterations;
  }

  /**
   * @description 计算最优切片大小
   */
  calculateOptimalChunkSize(fileSize) {
    // 基于网络速度和延迟计算
    const baseChunkSize = 1024 * 1024; // 1MB基础大小
    const maxChunkSize = 10 * 1024 * 1024; // 10MB最大
    const minChunkSize = 256 * 1024; // 256KB最小

    let optimalSize;

    if (this.networkSpeed > 1000) { // 高速网络 > 1MB/s
      optimalSize = Math.min(maxChunkSize, fileSize / 50);
    } else if (this.networkSpeed > 100) { // 中速网络 100KB/s - 1MB/s
      optimalSize = Math.min(5 * 1024 * 1024, fileSize / 100);
    } else { // 低速网络 < 100KB/s
      optimalSize = Math.min(2 * 1024 * 1024, fileSize / 200);
    }

    // 延迟调整
    if (this.avgLatency > 500) {
      optimalSize *= 1.5; // 高延迟时增大切片
    } else if (this.avgLatency < 100) {
      optimalSize *= 0.8; // 低延迟时减小切片
    }

    return Math.max(minChunkSize, Math.min(maxChunkSize, optimalSize));
  }

  /**
   * @description 动态调整并发数
   */
  calculateOptimalConcurrency() {
    if (this.networkSpeed > 1000) {
      return Math.min(6, this.concurrency + 1);
    } else if (this.networkSpeed < 100) {
      return Math.max(1, this.concurrency - 1);
    }
    return this.concurrency;
  }
}

// 使用示例
const calculator = new OptimalChunkCalculator();
await calculator.measureNetworkSpeed();

const optimalChunkSize = calculator.calculateOptimalChunkSize(file.size);
const optimalConcurrency = calculator.calculateOptimalConcurrency();

console.log(`网络速度: ${calculator.networkSpeed.toFixed(2)} KB/s`);
console.log(`平均延迟: ${calculator.avgLatency.toFixed(2)} ms`);
console.log(`最优切片大小: ${(optimalChunkSize / 1024 / 1024).toFixed(2)} MB`);
console.log(`最优并发数: ${optimalConcurrency}`);
```

### 2. 内存使用优化

```javascript
/**
 * @description 内存优化的文件处理器
 */
class MemoryOptimizedFileProcessor {
  constructor(maxMemoryUsage = 100 * 1024 * 1024) { // 100MB
    this.maxMemoryUsage = maxMemoryUsage;
    this.currentMemoryUsage = 0;
    this.processingQueue = [];
    this.cache = new Map();
  }

  /**
   * @description 流式读取大文件
   */
  async *readFileInChunks(file, chunkSize) {
    let offset = 0;

    while (offset < file.size) {
      // 内存使用检查
      await this.waitForMemoryAvailable(chunkSize);

      const end = Math.min(offset + chunkSize, file.size);
      const chunk = file.slice(offset, end);

      this.currentMemoryUsage += chunk.size;

      yield {
        chunk,
        index: Math.floor(offset / chunkSize),
        start: offset,
        end: end,
        isLast: end === file.size
      };

      offset = end;
    }
  }

  /**
   * @description 等待内存可用
   */
  async waitForMemoryAvailable(requiredSize) {
    while (this.currentMemoryUsage + requiredSize > this.maxMemoryUsage) {
      await new Promise(resolve => setTimeout(resolve, 100));
      this.cleanupMemory();
    }
  }

  /**
   * @description 清理内存
   */
  cleanupMemory() {
    // 清理缓存
    if (this.cache.size > 50) {
      const oldestKeys = Array.from(this.cache.keys()).slice(0, 10);
      oldestKeys.forEach(key => {
        const item = this.cache.get(key);
        this.currentMemoryUsage -= item.size;
        this.cache.delete(key);
      });
    }

    // 强制垃圾回收（如果可用）
    if (window.gc) {
      window.gc();
    }
  }

  /**
   * @description 释放切片内存
   */
  releaseChunkMemory(chunk) {
    this.currentMemoryUsage -= chunk.size;
  }

  /**
   * @description 获取内存使用情况
   */
  getMemoryInfo() {
    return {
      current: this.currentMemoryUsage,
      max: this.maxMemoryUsage,
      usage: (this.currentMemoryUsage / this.maxMemoryUsage * 100).toFixed(2) + '%',
      available: this.maxMemoryUsage - this.currentMemoryUsage
    };
  }
}
```

### 3. 网络重试策略

```javascript
/**
 * @description 智能重试策略
 */
class SmartRetryStrategy {
  constructor(options = {}) {
    this.maxRetries = options.maxRetries || 3;
    this.baseDelay = options.baseDelay || 1000;
    this.maxDelay = options.maxDelay || 30000;
    this.backoffFactor = options.backoffFactor || 2;
    this.jitterMax = options.jitterMax || 1000;
  }

  /**
   * @description 执行带重试的请求
   */
  async executeWithRetry(requestFn, context = {}) {
    let lastError;

    for (let attempt = 1; attempt <= this.maxRetries + 1; attempt++) {
      try {
        return await requestFn();
      } catch (error) {
        lastError = error;

        if (attempt > this.maxRetries) {
          throw error;
        }

        // 判断是否需要重试
        if (!this.shouldRetry(error, attempt)) {
          throw error;
        }

        // 计算延迟时间
        const delay = this.calculateDelay(attempt, error);
        console.log(`请求失败，${delay}ms后重试 (${attempt}/${this.maxRetries}):`, error.message);

        await this.delay(delay);
      }
    }

    throw lastError;
  }

  /**
   * @description 判断是否应该重试
   */
  shouldRetry(error, attempt) {
    // 网络错误或服务器错误才重试
    const retryableErrors = [
      'NetworkError',
      'TimeoutError',
      'AbortError'
    ];

    const retryableStatus = [408, 429, 500, 502, 503, 504];

    // 检查错误类型
    if (retryableErrors.includes(error.name)) {
      return true;
    }

    // 检查HTTP状态码
    if (error.status && retryableStatus.includes(error.status)) {
      return true;
    }

    // 检查网络连接
    if (!navigator.onLine) {
      return true;
    }

    return false;
  }

  /**
   * @description 计算重试延迟
   */
  calculateDelay(attempt, error) {
    // 基础指数退避
    let delay = this.baseDelay * Math.pow(this.backoffFactor, attempt - 1);

    // 根据错误类型调整
    if (error.status === 429) { // 限流
      delay *= 2;
    } else if (error.status >= 500) { // 服务器错误
      delay *= 1.5;
    }

    // 添加随机抖动
    const jitter = Math.random() * this.jitterMax;
    delay += jitter;

    return Math.min(delay, this.maxDelay);
  }

  /**
   * @description 延迟函数
   */
  delay(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

// 使用示例
const retryStrategy = new SmartRetryStrategy({
  maxRetries: 3,
  baseDelay: 1000,
  backoffFactor: 2
});

// 在上传器中使用
async function uploadChunkWithRetry(chunkData) {
  return retryStrategy.executeWithRetry(async () => {
    const formData = new FormData();
    formData.append('chunk', chunkData.chunk);
    formData.append('fileHash', chunkData.fileHash);
    formData.append('chunkIndex', chunkData.index);

    const response = await fetch('/api/upload/chunk', {
      method: 'POST',
      body: formData,
      signal: AbortSignal.timeout(30000) // 30秒超时
    });

    if (!response.ok) {
      const error = new Error(`Upload failed: ${response.statusText}`);
      error.status = response.status;
      throw error;
    }

    return response.json();
  });
}
```

## 常见面试问题

### 1. 大文件上传的技术难点有哪些？如何解决？

**答**：
大文件上传的主要技术难点包括：

1. **超时问题**：
   - 问题：大文件传输时间长，容易超时
   - 解决：文件切片，分块上传，每个切片独立处理

2. **内存占用**：
   - 问题：一次性读取大文件会占用大量内存
   - 解决：流式读取，分片处理，及时释放内存

3. **网络中断**：
   - 问题：网络不稳定导致上传失败
   - 解决：断点续传，记录上传进度，支持重新上传

4. **并发控制**：
   - 问题：过多并发请求可能导致服务器压力
   - 解决：控制并发数量，队列管理

```javascript
// 解决方案示例
const uploadConfig = {
  chunkSize: 2 * 1024 * 1024,    // 2MB切片
  concurrency: 3,                // 并发数控制
  retryTimes: 3,                // 重试次数
  timeout: 30000                 // 超时时间
};

// 内存优化
const memoryManager = {
  maxUsage: 100 * 1024 * 1024,  // 最大内存100MB
  cleanup() {
    // 定期清理不用的切片缓存
  }
};
```

### 2. 如何实现断点续传功能？

**答**：
断点续传的核心是记录和恢复上传进度：

1. **进度记录**：
   - 前端：使用localStorage记录已上传的切片信息
   - 后端：在服务器保存切片上传状态

2. **状态查询**：
   - 上传前查询已上传的切片列表
   - 跳过已上传的切片，只上传缺失部分

3. **文件标识**：
   - 使用文件hash作为唯一标识
   - 确保同一文件在不同时间上传时能正确识别

```javascript
// 断点续传实现
class ResumeableUploader {
  async resumeUpload(file) {
    // 1. 计算文件hash
    const fileHash = await this.calculateHash(file);

    // 2. 查询已上传切片
    const uploadedChunks = await this.getUploadedChunks(fileHash);

    // 3. 创建切片列表
    const allChunks = this.createChunks(file);

    // 4. 过滤已上传的切片
    const pendingChunks = allChunks.filter(
      chunk => !uploadedChunks.includes(chunk.index)
    );

    // 5. 上传剩余切片
    await this.uploadChunks(pendingChunks);

    // 6. 合并文件
    await this.mergeFile(fileHash, file.name);
  }

  // 本地状态管理
  saveProgress(fileHash, chunkIndex) {
    const key = `upload_${fileHash}`;
    const progress = JSON.parse(localStorage.getItem(key) || '[]');
    progress.push(chunkIndex);
    localStorage.setItem(key, JSON.stringify(progress));
  }

  getLocalProgress(fileHash) {
    const key = `upload_${fileHash}`;
    return JSON.parse(localStorage.getItem(key) || '[]');
  }
}
```

### 3. 如何优化大文件上传的性能？

**答**：
性能优化可以从多个维度进行：

1. **切片大小优化**：
   - 根据网络速度动态调整切片大小
   - 高速网络使用大切片，低速网络使用小切片

2. **并发控制**：
   - 合理设置并发数，避免过多并发导致服务器压力
   - 根据网络状况动态调整并发数

3. **缓存策略**：
   - 客户端缓存已计算的文件hash
   - 服务端缓存切片状态信息

4. **压缩优化**：
   - 对可压缩文件进行压缩传输
   - 使用gzip或brotli压缩

```javascript
// 性能优化配置
const performanceConfig = {
  // 动态切片大小
  calculateChunkSize(networkSpeed) {
    if (networkSpeed > 1000) return 5 * 1024 * 1024; // 5MB
    if (networkSpeed > 100) return 2 * 1024 * 1024;  // 2MB
    return 1024 * 1024; // 1MB
  },

  // 动态并发数
  calculateConcurrency(networkSpeed, serverLoad) {
    let concurrency = Math.min(6, Math.max(1, Math.floor(networkSpeed / 200)));
    if (serverLoad > 0.8) concurrency = Math.max(1, concurrency - 2);
    return concurrency;
  },

  // 智能重试
  retryStrategy: {
    maxRetries: 3,
    backoffFactor: 1.5,
    maxDelay: 30000
  }
};

// 网络状况监控
class NetworkMonitor {
  constructor() {
    this.speed = 0;
    this.latency = 0;
  }

  async measureSpeed() {
    const start = performance.now();
    const response = await fetch('/api/ping');
    const end = performance.now();

    this.latency = end - start;
    // 根据响应时间估算网络速度
    this.speed = this.estimateSpeed(this.latency);
  }

  estimateSpeed(latency) {
    // 基于延迟估算速度的简单算法
    if (latency < 50) return 1000; // 高速
    if (latency < 200) return 500; // 中速
    return 100; // 低速
  }
}
```

### 4. 大文件上传的安全性如何保证？

**答**：
大文件上传的安全性需要从多个方面考虑：

1. **文件类型验证**：
   - 前端和后端双重验证文件类型
   - 使用文件头信息验证，不仅依赖扩展名

2. **文件大小限制**：
   - 设置单文件和总上传大小限制
   - 防止恶意大文件攻击

3. **病毒扫描**：
   - 上传完成后进行病毒扫描
   - 集成第三方安全服务

4. **访问控制**：
   - 用户身份验证和授权
   - 上传令牌验证

```javascript
// 安全验证实现
class SecurityValidator {
  // 文件类型验证
  validateFileType(file, allowedTypes) {
    // 1. 扩展名检查
    const extension = file.name.split('.').pop().toLowerCase();
    if (!allowedTypes.extensions.includes(extension)) {
      throw new Error('不支持的文件类型');
    }

    // 2. MIME类型检查
    if (!allowedTypes.mimeTypes.includes(file.type)) {
      throw new Error('MIME类型不匹配');
    }

    // 3. 文件头检查
    return this.validateFileHeader(file, extension);
  }

  // 文件头验证
  async validateFileHeader(file, extension) {
    const headerMap = {
      'jpg': [0xFF, 0xD8, 0xFF],
      'png': [0x89, 0x50, 0x4E, 0x47],
      'pdf': [0x25, 0x50, 0x44, 0x46]
    };

    const expectedHeader = headerMap[extension];
    if (!expectedHeader) return true;

    const header = await this.readFileHeader(file, expectedHeader.length);
    return this.compareHeaders(header, expectedHeader);
  }

  // 大小限制验证
  validateFileSize(file, maxSize) {
    if (file.size > maxSize) {
      throw new Error(`文件大小超过限制 ${this.formatSize(maxSize)}`);
    }
  }

  // 恶意文件检测
  async scanForMalware(fileHash) {
    const response = await fetch('/api/security/scan', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ fileHash })
    });

    const result = await response.json();
    if (result.threat) {
      throw new Error('检测到恶意文件');
    }
  }
}

// 使用示例
const validator = new SecurityValidator();

// 上传前验证
const allowedTypes = {
  extensions: ['jpg', 'png', 'pdf', 'mp4'],
  mimeTypes: ['image/jpeg', 'image/png', 'application/pdf', 'video/mp4']
};

try {
  validator.validateFileType(file, allowedTypes);
  validator.validateFileSize(file, 500 * 1024 * 1024); // 500MB

  // 上传后扫描
  await validator.scanForMalware(fileHash);
} catch (error) {
  console.error('安全验证失败:', error.message);
}
```

### 5. 如何监控和分析大文件上传的性能？

**答**：
性能监控需要收集关键指标并进行分析：

1. **关键指标**：
   - 上传速度、成功率、错误率
   - 切片上传时间、重试次数
   - 服务器响应时间、资源使用率

2. **数据收集**：
   - 前端埋点收集用户行为数据
   - 后端日志记录详细的操作信息

3. **实时监控**：
   - 实时显示上传进度和状态
   - 错误预警和自动重试

```javascript
// 性能监控实现
class UploadMonitor {
  constructor() {
    this.metrics = {
      startTime: 0,
      endTime: 0,
      totalSize: 0,
      uploadedSize: 0,
      chunkTimes: [],
      errorCount: 0,
      retryCount: 0
    };
  }

  // 开始监控
  startMonitoring(file) {
    this.metrics.startTime = performance.now();
    this.metrics.totalSize = file.size;
    this.sendMetric('upload_start', {
      fileName: file.name,
      fileSize: file.size,
      timestamp: Date.now()
    });
  }

  // 记录切片性能
  recordChunkUpload(chunkIndex, size, duration, success) {
    this.metrics.chunkTimes.push({
      index: chunkIndex,
      size: size,
      duration: duration,
      success: success,
      speed: size / (duration / 1000) // bytes/second
    });

    if (success) {
      this.metrics.uploadedSize += size;
    } else {
      this.metrics.errorCount++;
    }

    // 发送实时数据
    this.sendMetric('chunk_upload', {
      chunkIndex,
      size,
      duration,
      success,
      timestamp: Date.now()
    });
  }

  // 记录重试
  recordRetry(chunkIndex) {
    this.metrics.retryCount++;
    this.sendMetric('chunk_retry', {
      chunkIndex,
      timestamp: Date.now()
    });
  }

  // 完成监控
  endMonitoring(success) {
    this.metrics.endTime = performance.now();
    const totalDuration = this.metrics.endTime - this.metrics.startTime;
    const averageSpeed = this.metrics.totalSize / (totalDuration / 1000);

    const summary = {
      success: success,
      totalDuration: totalDuration,
      averageSpeed: averageSpeed,
      successRate: (this.metrics.uploadedSize / this.metrics.totalSize) * 100,
      errorRate: (this.metrics.errorCount / this.metrics.chunkTimes.length) * 100,
      retryCount: this.metrics.retryCount
    };

    this.sendMetric('upload_complete', summary);
    return summary;
  }

  // 发送监控数据
  sendMetric(eventType, data) {
    // 发送到监控系统
    fetch('/api/metrics', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        event: eventType,
        data: data,
        userAgent: navigator.userAgent,
        timestamp: Date.now()
      })
    }).catch(error => {
      console.warn('监控数据发送失败:', error);
    });
  }

  // 获取实时统计
  getRealTimeStats() {
    const successfulChunks = this.metrics.chunkTimes.filter(c => c.success);
    const avgSpeed = successfulChunks.length > 0
      ? successfulChunks.reduce((sum, c) => sum + c.speed, 0) / successfulChunks.length
      : 0;

    return {
      progress: (this.metrics.uploadedSize / this.metrics.totalSize) * 100,
      averageSpeed: avgSpeed,
      estimatedTimeRemaining: this.calculateETA(avgSpeed),
      errorRate: (this.metrics.errorCount / this.metrics.chunkTimes.length) * 100,
      currentSpeed: this.getCurrentSpeed()
    };
  }

  // 计算预计剩余时间
  calculateETA(avgSpeed) {
    const remainingSize = this.metrics.totalSize - this.metrics.uploadedSize;
    return avgSpeed > 0 ? remainingSize / avgSpeed : 0;
  }

  // 获取当前速度（最近5个切片的平均速度）
  getCurrentSpeed() {
    const recentChunks = this.metrics.chunkTimes.slice(-5).filter(c => c.success);
    if (recentChunks.length === 0) return 0;

    return recentChunks.reduce((sum, c) => sum + c.speed, 0) / recentChunks.length;
  }
}

// 使用示例
const monitor = new UploadMonitor();

// 在上传器中集成监控
class MonitoredUploader extends LargeFileUploader {
  async upload(file, options = {}) {
    monitor.startMonitoring(file);

    try {
      const result = await super.upload(file, options);
      monitor.endMonitoring(true);
      return result;
    } catch (error) {
      monitor.endMonitoring(false);
      throw error;
    }
  }

  async uploadChunk(chunkData) {
    const startTime = performance.now();

    try {
      const result = await super.uploadChunk(chunkData);
      const duration = performance.now() - startTime;
      monitor.recordChunkUpload(chunkData.index, chunkData.chunk.size, duration, true);
      return result;
    } catch (error) {
      const duration = performance.now() - startTime;
      monitor.recordChunkUpload(chunkData.index, chunkData.chunk.size, duration, false);
      monitor.recordRetry(chunkData.index);
      throw error;
    }
  }
}
```

## 总结

大文件上传是一个复杂的技术挑战，需要综合考虑性能、稳定性、安全性等多个方面。核心技术包括文件切片、断点续传、并发控制等。在实际项目中，还需要根据具体的业务场景和技术栈进行优化和调整。

关键要点：
1. **文件切片**：将大文件分割为小块，分别处理
2. **断点续传**：记录上传进度，支持中断后继续
3. **并发控制**：合理控制并发数，平衡性能和资源
4. **错误处理**：智能重试策略，提高成功率
5. **性能监控**：实时监控上传状态，及时发现问题
6. **安全防护**：多层验证，确保文件安全性